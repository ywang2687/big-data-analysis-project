{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e99a1d-7656-4a3d-8d37-1728186507f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyarrow as pa\n",
    "import pyarrow.fs\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7006bbbf-8681-4ee0-84ee-69ed4b2818e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "Present Capacity: 7696498688 (7.17 GB)\n",
      "DFS Remaining: 7438815232 (6.93 GB)\n",
      "DFS Used: 257683456 (245.75 MB)\n",
      "DFS Used%: 3.35%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 167\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 91\n",
      "\tMissing blocks (with replication factor 1): 91\n",
      "\tLow redundancy blocks with highest priority to recover: 167\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 172.18.0.5:9866 (project-4-p4_yw_ym-dn-2.project-4-p4_yw_ym_default)\n",
      "Hostname: 43dc2ad30e77\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 257683456 (245.75 MB)\n",
      "Non DFS Used: 18107777024 (16.86 GB)\n",
      "DFS Remaining: 7438815232 (6.93 GB)\n",
      "DFS Used%: 1.00%\n",
      "DFS Remaining%: 28.81%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Fri Mar 15 00:22:56 GMT 2024\n",
      "Last Block Report: Thu Mar 14 23:07:59 GMT 2024\n",
      "Num of Blocks: 243\n",
      "\n",
      "\n",
      "Dead datanodes (1):\n",
      "\n",
      "Name: 172.18.0.4:9866 (172.18.0.4)\n",
      "Hostname: 595de693472a\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 272333716 (259.72 MB)\n",
      "Non DFS Used: 18091762796 (16.85 GB)\n",
      "DFS Remaining: 7440179200 (6.93 GB)\n",
      "DFS Used%: 1.05%\n",
      "DFS Remaining%: 28.81%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Thu Mar 14 23:10:53 GMT 2024\n",
      "Last Block Report: Thu Mar 14 23:07:59 GMT 2024\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!hdfs dfsadmin -fs hdfs://boss:9000 -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be01afc-2074-43e3-8c81-685ab75cf2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FileStatus': [{'accessTime': 1710461499049,\n",
       "   'blockSize': 1048576,\n",
       "   'childrenNum': 0,\n",
       "   'fileId': 16386,\n",
       "   'group': 'supergroup',\n",
       "   'length': 174944099,\n",
       "   'modificationTime': 1710457772795,\n",
       "   'owner': 'root',\n",
       "   'pathSuffix': '',\n",
       "   'permission': '644',\n",
       "   'replication': 1,\n",
       "   'storagePolicy': 0,\n",
       "   'type': 'FILE'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://boss:9870/webhdfs/v1/single.csv?op=LISTSTATUS\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "file = response.json()\n",
    "file_dict = file[\"FileStatuses\"]\n",
    "file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef882aa-9ffc-4b15-8ab8-c5d04fe343ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174944099"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = file_dict['FileStatus'][0]['blockSize']\n",
    "length = file_dict['FileStatus'][0]['length']\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7234b06b-f648-4fe6-9eaf-f8cb257ba588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'43dc2ad30e77': 76, 'lost': 91}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q9\n",
    "block_distribution = {}\n",
    "safe_list = []\n",
    "\n",
    "for i in range(0, length, size):\n",
    "    offset = str(i)\n",
    "    url = \"http://boss:9870/webhdfs/v1/single.csv?op=OPEN&offset=\" + offset + \"&noredirect=true\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code  == 403:\n",
    "        block_distribution['lost'] = block_distribution.get('lost', 0) + 1\n",
    "    else:\n",
    "        safe_list.append(i)\n",
    "        response = response.json()\n",
    "        #print(response)\n",
    "        loc = response['Location']\n",
    "        #print(loc)\n",
    "        datanode_id = loc.split(\"//\")[1].split(\"/\")[0].split(\":\")[0]\n",
    "        block_distribution[datanode_id] = block_distribution.get(datanode_id, 0) + 1\n",
    "\n",
    "block_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1961d396-bd03-4311-8427-b4602e59ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 00:22:59,427 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "hdfs = pa.fs.HadoopFileSystem(\"boss\", 9000)\n",
    "file_path = '/single.csv'  \n",
    "f = hdfs.open_input_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9774aba9-4531-49ce-a692-e9afc929989b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203051"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q10\n",
    "#create a safe list of offset\n",
    "#interate through that list\n",
    "#see if single family is existed in them\n",
    "count = 0\n",
    "for i in safe_list:\n",
    "    #print(i)\n",
    "    with hdfs.open_input_file(file_path) as file_obj:      \n",
    "        content = f.read_at(size,i)\n",
    "        count += content.count(b\"Single Family\")\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34428a9c-f5dc-4c7f-b826-9d8b9139f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1fbe2-788d-4aa7-9736-27ca3489b6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
